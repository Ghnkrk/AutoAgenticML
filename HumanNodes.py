



import json


def HumanReview_PreprocessConfig(state):

        # Safety check: ensure analysis evaluator has run
        config = state.get("analysisEvaluator_response")
        if not config:
            error_msg = (
                "\n‚ùå ERROR: analysisEvaluator_response missing from state!\n\n"
                "The orchestrator skipped the 'analysis_evaluator' stage.\n"
                "Correct sequence: analysis ‚Üí analysis_evaluator ‚Üí human\n\n"
                "This is an LLM routing error. The orchestrator should route:\n"
                f"  Current from_stage: {state.get('from_stage')}\n"
                "  Expected to_stage: analysis_evaluator\n"
                f"  Actual to_stage: {state.get('to_stage')}\n"
            )
            raise ValueError(error_msg)

        print("\n===== PRE-ANALYSIS CONFIG =====")
        print(json.dumps(config, indent=2))

        choice = input("\nEdit config? (y/n): ").strip().lower()
        if choice != "y":
            print("Config accepted as-is.")
            return {
                "analysisEvaluator_response": config,
                "num_columns": state["num_columns"],  # Preserve columns
                "cat_columns": state["cat_columns"],  # Preserve columns
                "from_stage": "human"
            }

        # Example edits (keep this minimal)
        drop_extra = input(
            "Add columns to DROP (comma-separated, blank to skip): "
        ).strip()

        if drop_extra:
            cols = [c.strip() for c in drop_extra.split(",")]
            config["features"]["drop"].extend(cols)
            # ensure consistency
            config["features"]["keep"] = [
                c for c in config["features"]["keep"] if c not in cols
            ]
            config["features"]["uncertain"] = [
                c for c in config["features"]["uncertain"] if c not in cols
            ]

        scaler = input(
            "Scaler (standard/minmax/robust, blank to keep current): "
        ).strip()

        if scaler:
            config["scaling"]["method"] = scaler
        
        # Add PCA editing
        pca_choice = input(
            f"Use PCA? (current: {config['dimensionality_reduction']['use_pca']}, y/n/blank to keep): "
        ).strip().lower()
        
        if pca_choice == "y":
            config["dimensionality_reduction"]["use_pca"] = True
            variance = input(
                f"PCA variance threshold (current: {config['dimensionality_reduction']['variance_threshold']}, blank to keep): "
            ).strip()
            if variance:
                try:
                    config["dimensionality_reduction"]["variance_threshold"] = float(variance)
                except ValueError:
                    print("  ‚ö†Ô∏è  Invalid variance value, keeping current")
        elif pca_choice == "n":
            config["dimensionality_reduction"]["use_pca"] = False

        print("\nUpdated config:")
        print(json.dumps(config, indent=2))

        confirm = input("\nConfirm config? (y/n): ").strip().lower()
        if confirm != "y":
            raise RuntimeError("Human rejected config")

        return {
            "analysisEvaluator_response": config,
            "num_columns": state["num_columns"],  # Preserve columns
            "cat_columns": state["cat_columns"],  # Preserve columns
            "from_stage": "human",
            "logs": [f"Human reviewed analysis config: {config}"]
        }

def HumanReview_ModelSelection(state):
        """
        Human review checkpoint for model selection.
        Reviews models generated by ModelDesignNode and allows:
        - Dropping unwanted models
        - Editing hyperparameters
        - Changing evaluation metric
        """
        assert state["from_stage"] in ["model_design", "human_evaluation_retrain"], f"Invalid stage: {state['from_stage']}"
        
        # Get model design response from state
        design_response = state.get("modelDesign_response")
        if not design_response:
            raise ValueError("modelDesign_response missing from state")
        
        models = design_response["models"]  # List of ModelSpec dicts
        metric = design_response.get("primary_metric", "accuracy")
        notes = design_response.get("notes", "")
        
        # Get task_type from state
        task_type = state.get("task_type") or state.get("analysisEvaluator_response", {}).get("task_type", "unknown")

        print("\n===== MODEL SELECTION REVIEW =====")
        print(f"Task: {task_type}")
        print(f"Primary Metric: {metric}")
        print(f"Notes: {notes}\n")

        for i, m in enumerate(models):
            print(f"[{i}] {m['name']}")
            # params is a list of {"name": ..., "value": ...}
            params_str = ", ".join([f"{p['name']}={p['value']}" for p in m['params']])
            print(f"    params     : {params_str}")
            print(f"    rationale  : {m['rationale']}\n")

        # --- Drop models ---
        drop = input(
            "Enter model indices to DROP (comma-separated), or press Enter to keep all: "
        ).strip()

        if drop:
            drop_ids = {int(i) for i in drop.split(",")}
            models = [m for i, m in enumerate(models) if i not in drop_ids]
            print(f"Dropped {len(drop_ids)} model(s). Remaining: {len(models)}")

        # --- Edit parameters (MULTI-PARAM SAFE) ---
        edit_any = input("\nEdit any model parameters? (y/n): ").lower()
        if edit_any == "y":
            for m in models:
                print(f"\n--- Editing {m['name']} ---")
                params_str = ", ".join([f"{p['name']}={p['value']}" for p in m['params']])
                print(f"Current params: {params_str}")

                edit = input("Edit this model's parameters? (y/n): ").lower()
                if edit != "y":
                    continue

                # Convert list ‚Üí dict for easier editing
                param_map = {p["name"]: p["value"] for p in m["params"]}

                while True:
                    name = input("  Param name to ADD/UPDATE (blank to finish): ").strip()
                    if not name:
                        break
                    value_str = input(f"  New value for '{name}': ").strip()
                    
                    # Try to parse as number, bool, or keep as string
                    if value_str.lower() == "none":
                        value = None
                    elif value_str.lower() in ["true", "false"]:
                        value = value_str.lower() == "true"
                    else:
                        try:
                            value = float(value_str)
                        except ValueError:
                            value = value_str
                    
                    param_map[name] = value
                    print(f"  ‚úì Set {name} = {value}")

                # Convert back to list format
                m["params"] = [{"name": k, "value": v} for k, v in param_map.items()]

        # --- Change metric ---
        new_metric = input(
            f"\nChange evaluation metric? (current: {metric}, press Enter to keep): "
        ).strip()
        if new_metric:
            metric = new_metric
            print(f"‚úì Metric changed to: {metric}")

        # --- Final confirmation ---
        print("\n===== FINAL MODEL SELECTION =====")
        for i, m in enumerate(models):
            params_str = ", ".join([f"{p['name']}={p['value']}" for p in m['params']])
            print(f"[{i}] {m['name']}: {params_str}")
        print(f"\nMetric: {metric}\n")
        
        confirm = input("Confirm model selection and continue? (y/n): ").lower()
        if confirm != "y":
            raise RuntimeError("Model selection rejected by human")

        print("\n‚úì Model selection approved.\n")

        # Update the design response with human modifications
        updated_design_response = {
            "models": models,
            "primary_metric": metric,
            "notes": notes + " [Human reviewed and approved]"
        }

        return {
            "modelDesign_response": updated_design_response,
            "from_stage": "human_model_selection",
            "logs": [f"Human review completed: {len(models)} model(s) approved with metric={metric}"]
        }

def HumanReview_ModelEvaluation(state):
        """
        Human review checkpoint for model evaluation results.
        User decides: accept models or retrain.
        """
        assert state["to_stage"] == "human_evaluation", "Invalid stage"
        
        # Get evaluation results and agent recommendation
        evaluation_results = state.get("evaluation_results", {})
        agent_response = state.get("evaluator_agent_response", {})
        
        print("\n" + "="*80)
        print("MODEL EVALUATION RESULTS")
        print("="*80)
        
        # Display model ranking
        print("\nModel Ranking:")
        for rank_info in evaluation_results.get("model_ranking", []):
            print(f"  {rank_info['rank']}. {rank_info['model_name']}: {rank_info['score']:.4f}")
        
        print(f"\nBest Model: {evaluation_results.get('best_model')}")
        print(f"Best Score: {evaluation_results.get('best_score', 0):.4f}")
        print(f"Confidence: {evaluation_results.get('confidence')}")
        
        # Display warnings
        warnings = evaluation_results.get("warnings", [])
        if warnings:
            print(f"\n‚ö†Ô∏è  Warnings ({len(warnings)}):")
            for w in warnings:
                print(f"  - {w}")
        
        # Display agent recommendation
        print(f"\nü§ñ Agent Decision: {agent_response.get('decision')}")
        print(f"Reasoning: {agent_response.get('reasoning')}")
        
        if agent_response.get('suggested_improvements'):
            print(f"\nSuggested Improvements:\n{agent_response.get('suggested_improvements')}")
        
        print("\n" + "="*80)
        
        # User decision
        decision = input("\nAccept models or retrain? (accept/retrain): ").lower().strip()
        
        if decision == "retrain":
            print("‚úì Retraining selected - returning to model selection")
            return {
                "from_stage": "human_evaluation_retrain",  # Special flag for retrain loop
                "logs": ["Human review: User requested retraining"]
            }
        else:
            print("‚úì Models accepted - proceeding to inference decision")
            return {
                "from_stage": "human_evaluation",
                "logs": ["Human review: Models accepted"]
            }
    
def HumanReview_InferenceDecision(state):
        """
        Human review checkpoint for inference decision.
        User decides: run inference or finish pipeline.
        If inference: select models and provide test dataset path.
        """
        assert state["to_stage"] == "human_inference", "Invalid stage"
        
        # Get trained models
        trained_models = state.get("trained_models", [])
        
        print("\n" + "="*80)
        print("INFERENCE DECISION")
        print("="*80)
        
        print(f"\n{len(trained_models)} trained model(s) available:")
        for i, m in enumerate(trained_models):
            print(f"  [{i}] {m['name']}")
        
        print("\nOptions:")
        print("  1. Run inference on test data")
        print("  2. Finish pipeline")
        
        choice = input("\nYour choice (1/2): ").strip()
        
        if choice == "2":
            print("‚úì Pipeline finished")
            return {
                "from_stage": "human_inference",
                "logs": ["Human review: Pipeline finished without inference"]
            }
        
        # User wants inference
        print("\n--- Inference Setup ---")
        
        # Select models for inference
        print(f"\nSelect models for inference (comma-separated indices, or 'all'):")
        selection = input("Models: ").strip().lower()
        
        if selection == "all":
            selected_models = [m["name"] for m in trained_models]
        else:
            indices = [int(i.strip()) for i in selection.split(",")]
            selected_models = [trained_models[i]["name"] for i in indices]
        
        print(f"‚úì Selected {len(selected_models)} model(s): {', '.join(selected_models)}")
        
        # Get test dataset path
        test_path = input("\nEnter test dataset path: ").strip()
        
        print(f"‚úì Test dataset: {test_path}")
        print("\n‚ö†Ô∏è  Note: Pipeline will now route to L0‚ÜíL1 for test preprocessing, then return for inference")
        
        return {
            "test_dataset_path": test_path,
            "selected_models_for_inference": selected_models,
            "return_to_inference": True,  # Flag for cross-phase routing
            "has_target": False,  # Test data has no target
            "from_stage": "human_inference",
            "logs": [f"Human review: Inference requested for {len(selected_models)} model(s)"]
        }